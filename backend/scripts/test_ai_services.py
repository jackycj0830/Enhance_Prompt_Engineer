"""
AIæœåŠ¡æµ‹è¯•è„šæœ¬
"""

import sys
import os
import asyncio
import time
sys.path.append(os.path.dirname(os.path.dirname(__file__)))

from app.services.ai_client import get_ai_client, AIClientError
from app.services.prompt_analyzer import get_prompt_analyzer

async def test_ai_client():
    """æµ‹è¯•AIå®¢æˆ·ç«¯åŸºç¡€åŠŸèƒ½"""
    print("ğŸ¤– æµ‹è¯•AIå®¢æˆ·ç«¯...")
    
    ai_client = get_ai_client()
    
    # æ£€æŸ¥å¯ç”¨æ¨¡å‹
    available_models = ai_client.get_available_models()
    print(f"ğŸ“‹ å¯ç”¨æ¨¡å‹: {available_models}")
    
    if not available_models:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„AIæ¨¡å‹ï¼Œè¯·æ£€æŸ¥APIå¯†é’¥é…ç½®")
        return False
    
    # æµ‹è¯•æ¯ä¸ªå¯ç”¨æ¨¡å‹
    test_prompt = "è¯·ç®€å•å›å¤'æµ‹è¯•æˆåŠŸ'æ¥ç¡®è®¤è¿æ¥æ­£å¸¸ã€‚"
    
    for model in available_models[:2]:  # åªæµ‹è¯•å‰ä¸¤ä¸ªæ¨¡å‹ä»¥èŠ‚çœæˆæœ¬
        print(f"\nğŸ” æµ‹è¯•æ¨¡å‹: {model}")
        
        try:
            start_time = time.time()
            response = await ai_client.generate_completion(
                prompt=test_prompt,
                model=model,
                temperature=0.1,
                max_tokens=50
            )
            
            print(f"âœ… å“åº”: {response.content}")
            print(f"â±ï¸  å“åº”æ—¶é—´: {response.response_time:.2f}ç§’")
            print(f"ğŸ”¢ Tokenä½¿ç”¨: {response.usage}")
            
        except AIClientError as e:
            print(f"âŒ æ¨¡å‹ {model} æµ‹è¯•å¤±è´¥: {e}")
            return False
        except Exception as e:
            print(f"âŒ æœªçŸ¥é”™è¯¯: {e}")
            return False
    
    return True

async def test_prompt_analyzer():
    """æµ‹è¯•æç¤ºè¯åˆ†æå™¨"""
    print("\nğŸ“Š æµ‹è¯•æç¤ºè¯åˆ†æå™¨...")
    
    ai_client = get_ai_client()
    analyzer = get_prompt_analyzer(ai_client)
    
    # æµ‹è¯•æç¤ºè¯
    test_prompts = [
        "å†™ä¸€ç¯‡æ–‡ç« ",  # ç®€å•æ¨¡ç³Šçš„æç¤ºè¯
        "è¯·ä½ ä½œä¸ºä¸€ä¸ªä¸“ä¸šçš„æŠ€æœ¯å†™ä½œä¸“å®¶ï¼Œä¸ºæˆ‘å†™ä¸€ç¯‡å…³äºäººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸåº”ç”¨çš„æŠ€æœ¯æ–‡ç« ã€‚æ–‡ç« åº”è¯¥åŒ…å«ä»¥ä¸‹è¦ç‚¹ï¼š1. AIåœ¨è¯Šæ–­ä¸­çš„åº”ç”¨ 2. æœºå™¨å­¦ä¹ åœ¨è¯ç‰©å‘ç°ä¸­çš„ä½œç”¨ 3. æœªæ¥å‘å±•è¶‹åŠ¿ã€‚æ–‡ç« é•¿åº¦æ§åˆ¶åœ¨1500å­—å·¦å³ï¼Œè¯­è¨€è¦ä¸“ä¸šä½†æ˜“æ‡‚ã€‚",  # è¯¦ç»†å…·ä½“çš„æç¤ºè¯
    ]
    
    for i, prompt in enumerate(test_prompts, 1):
        print(f"\nğŸ“ æµ‹è¯•æç¤ºè¯ {i}: {prompt[:50]}...")
        
        try:
            # æ‰§è¡Œåˆ†æ
            result = await analyzer.analyze_prompt(
                text=prompt,
                model=ai_client.get_available_models()[0] if ai_client.get_available_models() else "rule-based",
                use_ai_analysis=len(ai_client.get_available_models()) > 0
            )
            
            print(f"ğŸ“Š æ€»ä½“è¯„åˆ†: {result.metrics.overall_score}/100")
            print(f"ğŸ¯ è¯­ä¹‰æ¸…æ™°åº¦: {result.metrics.semantic_clarity}/100")
            print(f"ğŸ—ï¸  ç»“æ„å®Œæ•´æ€§: {result.metrics.structural_integrity}/100")
            print(f"ğŸ”— é€»è¾‘è¿è´¯æ€§: {result.metrics.logical_coherence}/100")
            print(f"ğŸ¯ å…·ä½“æ€§ç¨‹åº¦: {result.metrics.specificity_score}/100")
            print(f"ğŸ“‹ æŒ‡ä»¤æ¸…æ™°åº¦: {result.metrics.instruction_clarity}/100")
            print(f"â±ï¸  å¤„ç†æ—¶é—´: {result.processing_time:.2f}ç§’")
            print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {result.model_used}")
            
            if result.strengths:
                print(f"âœ… ä¼˜ç‚¹: {', '.join(result.strengths[:3])}")
            
            if result.weaknesses:
                print(f"âš ï¸  ç¼ºç‚¹: {', '.join(result.weaknesses[:3])}")
            
            if result.suggestions:
                print(f"ğŸ’¡ å»ºè®®: {', '.join(result.suggestions[:3])}")
        
        except Exception as e:
            print(f"âŒ åˆ†æå¤±è´¥: {e}")
            return False
    
    return True

async def test_batch_analysis():
    """æµ‹è¯•æ‰¹é‡åˆ†æåŠŸèƒ½"""
    print("\nğŸ”„ æµ‹è¯•æ‰¹é‡åˆ†æ...")
    
    ai_client = get_ai_client()
    
    if not ai_client.get_available_models():
        print("âš ï¸  è·³è¿‡æ‰¹é‡åˆ†ææµ‹è¯•ï¼ˆæ— å¯ç”¨AIæ¨¡å‹ï¼‰")
        return True
    
    test_prompts = [
        "ç¿»è¯‘è¿™æ®µæ–‡å­—",
        "æ€»ç»“è¿™ç¯‡æ–‡ç« çš„è¦ç‚¹",
        "åˆ†æè¿™ä¸ªæ•°æ®çš„è¶‹åŠ¿"
    ]
    
    try:
        start_time = time.time()
        responses = await ai_client.batch_generate(
            prompts=test_prompts,
            model=ai_client.get_available_models()[0],
            system_prompt="è¯·ç®€çŸ­å›å¤ï¼Œç¡®è®¤æ”¶åˆ°æŒ‡ä»¤ã€‚",
            max_concurrent=2
        )
        
        total_time = time.time() - start_time
        print(f"âœ… æ‰¹é‡åˆ†æå®Œæˆï¼Œå…±å¤„ç† {len(responses)} ä¸ªæç¤ºè¯")
        print(f"â±ï¸  æ€»è€—æ—¶: {total_time:.2f}ç§’")
        print(f"ğŸ“Š å¹³å‡å“åº”æ—¶é—´: {total_time/len(responses):.2f}ç§’")
        
        return True
    
    except Exception as e:
        print(f"âŒ æ‰¹é‡åˆ†æå¤±è´¥: {e}")
        return False

async def test_cost_calculation():
    """æµ‹è¯•æˆæœ¬è®¡ç®—åŠŸèƒ½"""
    print("\nğŸ’° æµ‹è¯•æˆæœ¬è®¡ç®—...")
    
    ai_client = get_ai_client()
    
    test_text = "è¿™æ˜¯ä¸€ä¸ªç”¨äºæµ‹è¯•tokenè®¡ç®—å’Œæˆæœ¬ä¼°ç®—çš„ç¤ºä¾‹æ–‡æœ¬ã€‚" * 10
    
    for model in ["gpt-3.5-turbo", "gpt-4", "claude-3-haiku", "claude-3-sonnet"]:
        token_count = ai_client.count_tokens(test_text, model)
        print(f"ğŸ“ æ¨¡å‹ {model}: {token_count} tokens")
    
    return True

def check_environment():
    """æ£€æŸ¥ç¯å¢ƒé…ç½®"""
    print("ğŸ” æ£€æŸ¥ç¯å¢ƒé…ç½®...")
    
    openai_key = os.getenv("OPENAI_API_KEY")
    anthropic_key = os.getenv("ANTHROPIC_API_KEY")
    
    print(f"ğŸ”‘ OpenAI API Key: {'âœ… å·²é…ç½®' if openai_key else 'âŒ æœªé…ç½®'}")
    print(f"ğŸ”‘ Anthropic API Key: {'âœ… å·²é…ç½®' if anthropic_key else 'âŒ æœªé…ç½®'}")
    
    if not openai_key and not anthropic_key:
        print("âš ï¸  è­¦å‘Š: æ²¡æœ‰é…ç½®ä»»ä½•AIæœåŠ¡APIå¯†é’¥")
        print("ğŸ’¡ è¯·åœ¨.envæ–‡ä»¶ä¸­é…ç½®OPENAI_API_KEYæˆ–ANTHROPIC_API_KEY")
        return False
    
    return True

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ§ª AIæœåŠ¡æµ‹è¯•å¼€å§‹")
    print("=" * 50)
    
    # æ£€æŸ¥ç¯å¢ƒ
    if not check_environment():
        print("\nâŒ ç¯å¢ƒæ£€æŸ¥å¤±è´¥ï¼Œè¯·é…ç½®APIå¯†é’¥åé‡è¯•")
        return
    
    # æµ‹è¯•AIå®¢æˆ·ç«¯
    if not await test_ai_client():
        print("\nâŒ AIå®¢æˆ·ç«¯æµ‹è¯•å¤±è´¥")
        return
    
    # æµ‹è¯•æç¤ºè¯åˆ†æå™¨
    if not await test_prompt_analyzer():
        print("\nâŒ æç¤ºè¯åˆ†æå™¨æµ‹è¯•å¤±è´¥")
        return
    
    # æµ‹è¯•æ‰¹é‡åˆ†æ
    if not await test_batch_analysis():
        print("\nâŒ æ‰¹é‡åˆ†ææµ‹è¯•å¤±è´¥")
        return
    
    # æµ‹è¯•æˆæœ¬è®¡ç®—
    if not await test_cost_calculation():
        print("\nâŒ æˆæœ¬è®¡ç®—æµ‹è¯•å¤±è´¥")
        return
    
    print("\n" + "=" * 50)
    print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼AIæœåŠ¡é›†æˆæˆåŠŸ")
    print("\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
    print("   - åœ¨ç”Ÿäº§ç¯å¢ƒä¸­è®¾ç½®åˆé€‚çš„APIé™æµ")
    print("   - ç›‘æ§APIä½¿ç”¨æˆæœ¬")
    print("   - å®šæœŸæ£€æŸ¥æ¨¡å‹å¯ç”¨æ€§")

if __name__ == "__main__":
    asyncio.run(main())
